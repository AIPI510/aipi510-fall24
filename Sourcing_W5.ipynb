{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data context and data sampling for the whole dataset\n"
      ],
      "metadata": {
        "id": "LATrNZgnV0If"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Data Context\n",
        "\n",
        "This repository contains heart rate variability (HRV) and COVID-19 related data, aiming to explore the potential links between physiological stress markers and COVID-19 incidence. The data is collected from users of the Welltory app, which tracks various physiological parameters through smartphone sensors and connected devices.\n",
        "\n",
        "- **Source:** Welltory app, a popular health tracking application that provides insights into HRV and stress levels.\n",
        "- **Purpose:** The primary purpose is to investigate the correlation between physiological stress, as indicated by HRV metrics, and COVID-19 infection rates. The data provides a unique perspective on how pandemic-related stress might manifest physiologically in users.\n",
        "- **Timeframe:** The data was collected from March 2020 to September 2020, capturing the early months of the COVID-19 pandemic.\n",
        "- **Population:** The dataset includes HRV and health data from thousands of users across multiple countries. The user base predominantly consists of individuals interested in tracking their health and fitness metrics.\n",
        "- **Key Variables:**\n",
        "  - **user_code:** An anonymized identifier for each user.\n",
        "  - **measurement_datetime:** The exact date and time when the measurement was recorded.\n",
        "  - **hrv_parameters:** Various HRV metrics such as RMSSD, pNN50, LF/HF ratio, and more.\n",
        "  - **covid_status:** Self-reported COVID-19 infection status.\n",
        "- **Limitations:** The dataset relies on self-reported COVID-19 status, which may be prone to reporting bias. Additionally, HRV data is influenced by multiple factors such as physical activity, sleep, and overall health, making it challenging to isolate the impact of COVID-19 alone.\n",
        "\n"
      ],
      "metadata": {
        "id": "vKUv9VyvV6bA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Sampling\n",
        "\n",
        "The data in this repository was collected from Welltory app users who opted in to share their anonymized data for research purposes. It includes all users who reported their COVID-19 status during the specified timeframe, along with their HRV measurements.\n",
        "\n",
        "- **Sampling Method:** Convenience sampling of Welltory app users who volunteered to share their data.\n",
        "- **Sample Size:** Data includes thousands of HRV measurements from thousands of unique users.\n",
        "- **Representativeness:** The sample primarily represents individuals interested in health and fitness tracking and may not be representative of the general population.\n",
        "- **Sampling Bias:** There is a potential for sampling bias as the data includes only those users who actively engage with the app and report their COVID-19 status. Additionally, there may be underreporting or misreporting of COVID-19 infection due to reliance on self-reports.\n",
        "\n"
      ],
      "metadata": {
        "id": "-_N2Pq-KWTvT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXoIGqB0i2Zg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import OrdinalEncoder,RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"/content/blood_pressure.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "aOHsaEcPjD8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data structure\n",
        "print(\"Data Structure:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "YLdavitKjWti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics for numerical columns\n",
        "df.describe()\n"
      ],
      "metadata": {
        "id": "JSOhKWnQkEeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Quality Assessment\n",
        "\n"
      ],
      "metadata": {
        "id": "ArBS0EozWKBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics for categorical columns\n",
        "for column in df.select_dtypes(include=['object']).columns:\n",
        "    print(f\"\\n{column} Value Counts:\")\n",
        "    print(df[column].value_counts())"
      ],
      "metadata": {
        "id": "31OTs_3nk9U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment** : Honestly, first thought I had before exploring this dataset was that I should most definitely drop the User_code since it appeared to have no intrinsic value. But, looking at the value_counts, there appears to be a pattern in the data."
      ],
      "metadata": {
        "id": "DckdbWB7kZ5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "3jvftiSekQzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of missing values in each column\n",
        "df.isnull().mean()\n"
      ],
      "metadata": {
        "id": "ENI4WfisivY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for duplicates\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "FtQnhHyai_3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for data type issues\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "iLqQ3-1djIpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show unique values in a column (user_code) as an example\n",
        "unique_values = df['user_code'].unique()\n",
        "print(\"\\nUnique Values in 'user_code' Column:\\n\", unique_values)"
      ],
      "metadata": {
        "id": "yLSZDmsBhNu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable relationships"
      ],
      "metadata": {
        "id": "iKKK6deplH0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Before we start visualizing for exploratory data analysis, lets make separate columns to encode the user_code data and also convert the measurement_datetime column to two seperate columns.\n"
      ],
      "metadata": {
        "id": "mvvVUQhqmzgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting measurement_datetime to datetime object\n",
        "df['measurement_datetime'] = pd.to_datetime(df['measurement_datetime'])\n",
        "\n",
        "# Extracting date and time into separate columns\n",
        "df['date'] = df['measurement_datetime'].dt.date\n",
        "df['time'] = df['measurement_datetime'].dt.time\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['time'] = pd.to_timedelta(df['time'].astype(str))\n"
      ],
      "metadata": {
        "id": "wBEgDiQ1my1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordinal encoding for 'user_code'\n",
        "encoder = OrdinalEncoder()\n",
        "df['user_code_ordinal'] = encoder.fit_transform(df[['user_code']])"
      ],
      "metadata": {
        "id": "Km0hT7Zumv5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping `measurement_datetime`\n",
        "\n",
        "df = df.drop(['measurement_datetime'], axis =1)\n",
        "\n",
        "# We're not dropping user_code, since we have a couple of visualizations to perform with that"
      ],
      "metadata": {
        "id": "6JeYhSM5laA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing it out\n",
        "df.info()"
      ],
      "metadata": {
        "id": "00CNcZD5ZE36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz_df = df.drop([\"user_code\", \"date\",\"time\",], axis =1)\n",
        "\n",
        "# Correlation analysis for df\n",
        "viz_df.corr()"
      ],
      "metadata": {
        "id": "D-OckMRxlMyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation matrix for numerical variables heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(viz_df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ZjPl643l9wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, a very high correlation between\n",
        "* `systolic` and `diastolic`\n",
        "* `functional_changes_index` and `diastolic`\n",
        "* `kerdo_vegetation` and `diastolic`\n",
        "\n",
        "* `functional_changes_index` and `systolic`\n",
        "* `circulatory_changes` and `systolic`\n",
        "\n",
        "* `robinson_index` and `functional_changes_index`\n",
        "\n",
        "* `robinson_index` and `circulatory_efficiency`\n"
      ],
      "metadata": {
        "id": "CsKEnwDGeSKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations for outlier detection"
      ],
      "metadata": {
        "id": "PmKAR8EZsIC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets visualize the scatterplot matrix, where we can get an idea about the outliers\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "pd.plotting.scatter_matrix(viz_df,diagonal='kde', figsize=(15, 15))\n",
        "plt.suptitle('Scatter Plot Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CWhkQMAtlyBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 1. Using Boxplots for Numerical Columns\n",
        "\n",
        "numerical_columns = df.select_dtypes(include=['number'])\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_columns, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NA9RwYChm9Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see the numerical columns have outliers to them, especially, `circulatory_efficiency` and `functional_changes_index`"
      ],
      "metadata": {
        "id": "1M8Zt3R24Izw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Calculating IQR to Detect Outliers\n",
        "for col in numerical_columns:\n",
        "    # Calculating Q1 (25th percentile) and Q3 (75th percentile)\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Defining outlier boundaries\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Identifying outliers\n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    print(f\"Outliers in {col}:\\n\", outliers[[col]])"
      ],
      "metadata": {
        "id": "BJ4reYAN0m6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Frequency Distribution for Categorical Columns\n",
        "categorical_columns = ['user_code']\n",
        "\n",
        "for col in categorical_columns:\n",
        "    value_counts = df[col].value_counts()\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.barplot(x=value_counts.index, y=value_counts.values, palette='viridis')\n",
        "    plt.title(f'Frequency Distribution of {col}')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "h4E0yJO71AGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Date and Time Columns\n",
        "print(f\"Date Range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "# Checking for unusual patterns in time, like all values being around midnight or similar patterns\n",
        "print(f\"Time Range: {df['time'].min()} to {df['time'].max()}\")"
      ],
      "metadata": {
        "id": "IQ8ok7Rq1ST6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Outliers\n",
        "\n"
      ],
      "metadata": {
        "id": "ts6DUi6I36kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While, I believe that its easier to solve for the numerical outliers problem by removing them outright, I want to try and minimize their impact by using a scaling method called RobustScaler.\n",
        "\n",
        "Prior to this, we are to deal with the INF and the NaN values\n"
      ],
      "metadata": {
        "id": "btvh7X6Z4ve8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dropping the 'time' column temporarily because it gets banded in with the numerical columns when we apply the command\n",
        "df_temp = df.drop(columns=['time'], errors='ignore')\n",
        "\n",
        "# Selecting numerical columns\n",
        "numerical_columns = df_temp.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Replacing Inf and -Inf with NaN\n",
        "df_temp.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Filling missing values with the median of each column\n",
        "df_temp[numerical_columns] = df_temp[numerical_columns].fillna(df_temp[numerical_columns].median())\n",
        "\n",
        "# Applying RobustScaler\n",
        "scaler = RobustScaler()\n",
        "df_temp[numerical_columns] = scaler.fit_transform(df_temp[numerical_columns])\n",
        "\n",
        "\n",
        "df_scaled = pd.concat([df_temp, df['time']], axis=1)\n",
        "\n",
        "df_scaled.head()"
      ],
      "metadata": {
        "id": "a3x5bM4K1atb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "CpwHKJJKDOgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new feature we'll be creating is called Pulse pressure, which is nothing but the `diastolic` pressure minus the `systolic` pressure."
      ],
      "metadata": {
        "id": "RIKe_pz0DskY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Pulse Pressure\n",
        "df['Pulse_Pressure'] = df['systolic'] - df['diastolic']\n",
        "\n",
        "# Check the result\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nFlrqqcF5anz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations"
      ],
      "metadata": {
        "id": "Xzxbb50JCP5v"
      }
    },
    {
      "source": [
        "# Systolic vs Diastolic time series analysis:\n",
        "\n",
        "df_scaled['diastolic'].plot(kind='line', figsize=(8, 4), title='')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "\n",
        "df_scaled['systolic'].plot(kind='line', figsize=(8, 4), title='systolic vs diastolic')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.legend()"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "_ydHTdT6CrJY"
      }
    },
    {
      "source": [
        "# Diastolic and Systolic time series:\n",
        "def _plot_series(series, series_name, series_index=0):\n",
        "  palette = list(sns.palettes.mpl_palette('Dark2'))\n",
        "  xs = series['date']\n",
        "  ys = series['systolic']\n",
        "\n",
        "  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\n",
        "df_sorted = df_scaled.sort_values('date', ascending=True)\n",
        "_plot_series(df_sorted, '')\n",
        "sns.despine(fig=fig, ax=ax)\n",
        "plt.xlabel('date')\n",
        "_ = plt.ylabel('systolic')"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "TUghhIrNCeXH"
      }
    },
    {
      "source": [
        "\n",
        "def _plot_series(series, series_name, series_index=0):\n",
        "  palette = list(sns.palettes.mpl_palette('Dark2'))\n",
        "  xs = series['date']\n",
        "  ys = series['diastolic']\n",
        "\n",
        "  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\n",
        "df_sorted = df_scaled.sort_values('date', ascending=True)\n",
        "_plot_series(df_sorted, '')\n",
        "sns.despine(fig=fig, ax=ax)\n",
        "plt.xlabel('date')\n",
        "_ = plt.ylabel('diastolic')"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "1s-uVYqrCWfu"
      }
    },
    {
      "source": [
        "# kerdo_vegetation_index vs circulatory_efficiency\n",
        "df_scaled.plot(kind='scatter', x='circulatory_efficiency', y='kerdo_vegetation_index', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "xRw09rCMB0U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled.head()"
      ],
      "metadata": {
        "id": "_d1HVmLuE8JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled.isnull().sum()"
      ],
      "metadata": {
        "id": "rldLjC0XFYdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction Method: Principal Component Analysis (PCA):"
      ],
      "metadata": {
        "id": "yFuha4ZrJ8bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the remaining non-numeric columns from df for pca\n",
        "df.drop(['date', 'time', 'user_code'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "5riVZntAOCuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Applying PCA w/ 2 components for graphing\n",
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(df)\n",
        "\n",
        "# Explained variance ratio\n",
        "print(\"EVR:\", pca.explained_variance_ratio_)\n",
        "\n",
        "# Plotting the principal components\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(principal_components[:, 0], principal_components[:, 1], alpha=0.5)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA on Processed Blood Pressure Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PXiaDjIeFYU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost all of the variance in the data is captured by the first principal component as we can see from the very large explained variance ratio of PC1 compared to PC2. We can see this visually in the scatter plot as their is far more variance and particularly extreme outliers with PC1 compared to PC2.\n"
      ],
      "metadata": {
        "id": "qX5k-k0pOpbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=df.columns)\n",
        "loadings"
      ],
      "metadata": {
        "id": "0_Hh-GG5NWZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a table to see which of the remaining variables have the most/least influence on PC1 and PC2."
      ],
      "metadata": {
        "id": "4pqChTSjPgMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data quality assessment report"
      ],
      "metadata": {
        "id": "gqAdbJnQTcp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the data is pretty incomplete many null values across many of the metrics. However, the most important metric, blood pressure is 100% complete. The data is clean and well formatted with no outside reasons to believe it is not high quality. The format of the data appears to be consistent and accurate and given it's linear as part of a larger and often used open research dataset, it can be trusted. Because the data is anonomyized, we do not know specifically how or where it was collected to ensure 100% accuracy."
      ],
      "metadata": {
        "id": "g5RSTU8KP_o7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgpCVOsoTYYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}